# Environment Configuration Template
# Copy this file to .env and fill in your values
# NEVER commit .env to version control

# ============================================================================
# Environment
# ============================================================================
ENVIRONMENT=development  # development, staging, production
DEBUG=true

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Supported providers: anthropic, openai, vllm, llama_cpp, openai_compatible
#
# Cloud providers require API keys.
# Local providers require base_url configuration.
#
# Recommended local models:
#   - GLM-4.7: Excellent code generation and reasoning
#   - MiniMax-M2.1: Strong multilingual and code capabilities

# Primary LLM provider
LLM_PROVIDER=anthropic  # Change to 'vllm' or 'llama_cpp' for local LLM

# Model name (provider-specific)
# Cloud: claude-sonnet-4-20250514, gpt-4o
# vLLM: THUDM/glm-4.7, MiniMaxAI/MiniMax-M2.1
# llama_cpp: /path/to/glm-4.7Q4_K_M.gguf, /path/to/MiniMax-M2.1.Q4_K_M.gguf
LLM_MODEL=claude-sonnet-4-20250514

# Base URL for local providers
# LLM_BASE_URL=http://localhost:8000   # vLLM default

# ============================================================================
# Cloud API Keys
# ============================================================================
# Anthropic Claude API (primary cloud provider)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI API (for LLM judge or alternative)
OPENAI_API_KEY=your_openai_api_key_here

# ============================================================================
# Local LLM Settings
# ============================================================================
# vLLM settings (recommended for production)
VLLM_BASE_URL=http://localhost:8000
# Start with: vllm serve THUDM/glm-4.7 --port 8000
# Or: vllm serve MiniMaxAI/MiniMax-M2.1 --port 8000

# llama.cpp settings (for edge deployment)
LLAMA_CPP_MODEL_PATH=/models/glm-4.7.Q4_K_M.gguf
# Alternative: /models/MiniMax-M2.1.Q4_K_M.gguf
LLAMA_CPP_N_CTX=4096
LLAMA_CPP_N_GPU_LAYERS=-1  # -1 = use all GPU layers

# OpenAI-compatible endpoint (for custom inference servers)
OPENAI_COMPATIBLE_BASE_URL=http://localhost:8080/v1

# ============================================================================
# Application Configuration
# ============================================================================
# Configuration file paths
CONFIG_PATH=fleet-manager/config.yaml
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# Agent settings
AGENT_MAX_TURNS=10
AGENT_MAX_RETRIES=3
AGENT_TIMEOUT=300  # seconds

# ============================================================================
# Verification Settings
# ============================================================================
# PLC Compiler paths (vendor-specific)
# SIEMENS_TIA_PATH=/path/to/TIA/Portal/
# ROCKWELL_STUDIO5000_PATH=/path/to/Studio5000/
# CODESYS_PATH=/path/to/CODESYS/

# Verification timeouts
VERIFICATION_COMPILER_TIMEOUT=300
VERIFICATION_SAFETY_TIMEOUT=600
VERIFICATION_SIMULATION_TIMEOUT=900

# Enable/disable verifications
VERIFICATION_ENABLE_SAFETY_VERIFIER=true
VERIFICATION_ENABLE_SIMULATION=false
VERIFICATION_ENABLE_LLM_JUDGE=true
VERIFICATION_JUDGE_CONFIDENCE_THRESHOLD=0.7

# Simulation settings
# PLCSIM_ENABLED=false
# PLCSIM_PATH=/path/to/PLCSIM/Advanced/

# ============================================================================
# Safety Settings
# ============================================================================
SAFETY_REQUIRE_SAFETY_REVIEW=true
SAFETY_REQUIRE_HUMAN_APPROVAL=true
SAFETY_ENHANCED_REVIEW_SIL=SIL-2

# ============================================================================
# MLflow Tracking (Observability)
# ============================================================================
TELEMETRY_ENABLED=true
TELEMETRY_MLFLOW_ENABLED=true
TELEMETRY_MLFLOW_TRACKING_URI=http://localhost:5000
TELEMETRY_MLFLOW_EXPERIMENT_NAME=background-coding-agents
TELEMETRY_TRACE_LLM_CALLS=true
TELEMETRY_TRACE_VERIFICATIONS=true

# ============================================================================
# Logging Configuration
# ============================================================================
LOG_LEVEL=INFO
LOG_FORMAT=json  # json or text
LOG_FILE=logs/fleet-manager.log
LOG_ROTATION=10MB
LOG_RETENTION=30  # days
LOG_INCLUDE_TIMESTAMP=true
LOG_INCLUDE_CORRELATION_ID=true
LOG_INCLUDE_CALLER=false

# ============================================================================
# API Server Configuration
# ============================================================================
API_HOST=0.0.0.0
API_PORT=8080
API_CORS_ORIGINS=*  # Restrict in production

# ============================================================================
# Performance Settings
# ============================================================================
# Concurrency limits
MAX_CONCURRENT_SITES=5
MAX_CONCURRENT_VERIFIERS=3

# Rate limiting
ANTHROPIC_RPM=50  # Requests per minute
OPENAI_RPM=60
LLM_RETRY_DELAY=1  # seconds
LLM_MAX_RETRIES=3

# ============================================================================
# Development/Testing Settings
# ============================================================================
# Mock mode (use mocked LLM/compiler responses for testing)
USE_MOCK_LLM=false
USE_MOCK_COMPILER=false
USE_MOCK_SAFETY_VERIFIER=false

# ============================================================================
# Cloud/Storage Configuration (optional)
# ============================================================================
# GCP (Google Cloud Platform)
# GCP_PROJECT_ID=your-gcp-project
# GCP_CREDENTIALS_PATH=/path/to/credentials.json
# GCP_BUCKET=your-gcs-bucket

# AWS (Amazon Web Services)
# AWS_ACCESS_KEY_ID=your-aws-access-key
# AWS_SECRET_ACCESS_KEY=your-aws-secret-key
# AWS_REGION=us-east-1
# S3_BUCKET=your-s3-bucket

# Azure
# AZURE_STORAGE_CONNECTION_STRING=your-connection-string
# AZURE_CONTAINER=your-container

# ============================================================================
# Security Settings
# ============================================================================
# Secret key for session management
# SECRET_KEY=your-secret-key-here

# Allowed hosts (comma-separated)
# ALLOWED_HOSTS=localhost,127.0.0.1

# ============================================================================
# Notifications (optional)
# ============================================================================
# Slack webhook for notifications
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Email settings
# SMTP_HOST=smtp.gmail.com
# SMTP_PORT=587
# SMTP_USER=your-email@example.com
# SMTP_PASSWORD=your-email-password
# SMTP_FROM=noreply@example.com
# ALERT_EMAIL=alerts@example.com

# ============================================================================
# Cache Settings
# ============================================================================
CACHE_ENABLED=true
CACHE_TTL=3600  # seconds
# CACHE_REDIS_URL=redis://localhost:6379/1

# ============================================================================
# Notes
# ============================================================================
# 1. Never commit .env files to version control
# 2. Add .env to .gitignore
# 3. Use different .env files for different environments
# 4. Rotate API keys regularly
# 5. For air-gapped environments, use vLLM or llama.cpp with GLM-4.7 or MiniMax-M2.1
