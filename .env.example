# Environment Configuration Template
# Copy this file to .env and fill in your values
# NEVER commit .env to version control

# ============================================================================
# Environment
# ============================================================================
ENVIRONMENT=development  # development, staging, production
DEBUG=true

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Supported providers: vllm, llama_cpp, anthropic, openai, openai_compatible
#
# RECOMMENDED: Use local providers (vllm, llama_cpp) for air-gapped environments
# Cloud providers require API keys and internet connection.
#
# Recommended local models:
#   - minimax-m2.1: Strong multilingual and code capabilities (vLLM)
#   - GLM-4.7: Excellent code generation and reasoning (vLLM/llama.cpp)
#   - MiniMax-M2.1: For llama.cpp GGUF format

# Primary LLM provider (LOCAL FIRST)
LLM_PROVIDER=vllm  # Use 'anthropic' or 'openai' for cloud

# Model name (provider-specific)
# Local vLLM: minimax-m2.1, THUDM/glm-4.7-chat, MiniMaxAI/MiniMax-M2.1
# Local llama.cpp: /path/to/glm-4.7-chat.Q4_K_M.gguf, /path/to/MiniMax-M2.1.Q4_K_M.gguf
# Cloud: claude-sonnet-4-20250514, gpt-4o
# MiniMax Cloud via Anthropic API: MiniMax-M2.1, MiniMax-M2.1-lightning, MiniMax-M2
LLM_MODEL=minimax-m2.1

# Temperature (0.0 for deterministic, higher for creative)
LLM_TEMPERATURE=0.0

# Base URL for local/custom providers
LLM_BASE_URL=http://localhost:8000   # vLLM default
# LLM_BASE_URL=https://api.minimax.io/anthropic  # MiniMax Cloud international
# LLM_BASE_URL=https://api.minimaxi.com/anthropic  # MiniMax Cloud China

# ============================================================================
# Cloud API Keys (OPTIONAL - only needed for cloud providers)
# ============================================================================
# Only set these if using cloud providers (LLM_PROVIDER=anthropic/openai)
# Not needed for local vLLM or llama.cpp

# Anthropic Claude API
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI API
# OPENAI_API_KEY=your_openai_api_key_here

# MiniMax Cloud via Anthropic-compatible API
# Get your key from: https://platform.minimax.io
# Also set: LLM_PROVIDER=anthropic, LLM_BASE_URL=https://api.minimax.io/anthropic
# ANTHROPIC_API_KEY=your_minimax_api_key_here

# ============================================================================
# Local LLM Settings (PRIMARY DEPLOYMENT)
# ============================================================================
# vLLM settings (recommended for production)
VLLM_BASE_URL=http://localhost:8000
# Start your local vLLM server:
#   vllm serve minimax-m2.1 --port 8000
#   vllm serve THUDM/glm-4.7-chat --port 8000
#   vllm serve MiniMaxAI/MiniMax-M2.1 --port 8000

# llama.cpp settings (for edge deployment)
LLAMA_CPP_MODEL_PATH=/models/glm-4.7.Q4_K_M.gguf
# Alternative: /models/MiniMax-M2.1.Q4_K_M.gguf
LLAMA_CPP_N_CTX=4096
LLAMA_CPP_N_GPU_LAYERS=-1  # -1 = use all GPU layers

# OpenAI-compatible endpoint (for custom inference servers)
OPENAI_COMPATIBLE_BASE_URL=http://localhost:8080/v1

# ============================================================================
# Application Configuration
# ============================================================================
# Configuration file paths
CONFIG_PATH=fleet-manager/config.yaml
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# Agent settings
AGENT_MAX_TURNS=10
AGENT_MAX_RETRIES=3
AGENT_TIMEOUT=300  # seconds

# ============================================================================
# Verification Settings
# ============================================================================
# PLC Compiler paths (vendor-specific)
# SIEMENS_TIA_PATH=/path/to/TIA/Portal/
# ROCKWELL_STUDIO5000_PATH=/path/to/Studio5000/
# CODESYS_PATH=/path/to/CODESYS/

# Verification timeouts
VERIFICATION_COMPILER_TIMEOUT=300
VERIFICATION_SAFETY_TIMEOUT=600
VERIFICATION_SIMULATION_TIMEOUT=900

# Enable/disable verifications
VERIFICATION_ENABLE_SAFETY_VERIFIER=true
VERIFICATION_ENABLE_SIMULATION=false
VERIFICATION_ENABLE_LLM_JUDGE=true
VERIFICATION_JUDGE_CONFIDENCE_THRESHOLD=0.7

# Simulation settings
# PLCSIM_ENABLED=false
# PLCSIM_PATH=/path/to/PLCSIM/Advanced/

# ============================================================================
# Safety Settings
# ============================================================================
SAFETY_REQUIRE_SAFETY_REVIEW=true
SAFETY_REQUIRE_HUMAN_APPROVAL=true
SAFETY_ENHANCED_REVIEW_SIL=SIL-2

# ============================================================================
# MLflow Tracking (Observability)
# ============================================================================
TELEMETRY_ENABLED=true
TELEMETRY_MLFLOW_ENABLED=true
TELEMETRY_MLFLOW_TRACKING_URI=http://localhost:5000
TELEMETRY_MLFLOW_EXPERIMENT_NAME=background-coding-agents
TELEMETRY_TRACE_LLM_CALLS=true
TELEMETRY_TRACE_VERIFICATIONS=true

# ============================================================================
# Logging Configuration
# ============================================================================
LOG_LEVEL=INFO
LOG_FORMAT=json  # json or text
LOG_FILE=logs/fleet-manager.log
LOG_ROTATION=10MB
LOG_RETENTION=30  # days
LOG_INCLUDE_TIMESTAMP=true
LOG_INCLUDE_CORRELATION_ID=true
LOG_INCLUDE_CALLER=false

# ============================================================================
# API Server Configuration
# ============================================================================
API_HOST=0.0.0.0
API_PORT=8080
API_CORS_ORIGINS=*  # Restrict in production

# ============================================================================
# Performance Settings
# ============================================================================
# Concurrency limits
MAX_CONCURRENT_SITES=5
MAX_CONCURRENT_VERIFIERS=3

# Rate limiting
ANTHROPIC_RPM=50  # Requests per minute
OPENAI_RPM=60
LLM_RETRY_DELAY=1  # seconds
LLM_MAX_RETRIES=3

# ============================================================================
# Development/Testing Settings
# ============================================================================
# Mock mode (use mocked LLM/compiler responses for testing)
USE_MOCK_LLM=false
USE_MOCK_COMPILER=false
USE_MOCK_SAFETY_VERIFIER=false

# ============================================================================
# Cloud/Storage Configuration (optional)
# ============================================================================
# GCP (Google Cloud Platform)
# GCP_PROJECT_ID=your-gcp-project
# GCP_CREDENTIALS_PATH=/path/to/credentials.json
# GCP_BUCKET=your-gcs-bucket

# AWS (Amazon Web Services)
# AWS_ACCESS_KEY_ID=your-aws-access-key
# AWS_SECRET_ACCESS_KEY=your-aws-secret-key
# AWS_REGION=us-east-1
# S3_BUCKET=your-s3-bucket

# Azure
# AZURE_STORAGE_CONNECTION_STRING=your-connection-string
# AZURE_CONTAINER=your-container

# ============================================================================
# Security Settings
# ============================================================================
# Secret key for session management
# SECRET_KEY=your-secret-key-here

# Allowed hosts (comma-separated)
# ALLOWED_HOSTS=localhost,127.0.0.1

# ============================================================================
# Notifications (optional)
# ============================================================================
# Slack webhook for notifications
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Email settings
# SMTP_HOST=smtp.gmail.com
# SMTP_PORT=587
# SMTP_USER=your-email@example.com
# SMTP_PASSWORD=your-email-password
# SMTP_FROM=noreply@example.com
# ALERT_EMAIL=alerts@example.com

# ============================================================================
# Cache Settings
# ============================================================================
CACHE_ENABLED=true
CACHE_TTL=3600  # seconds
# CACHE_REDIS_URL=redis://localhost:6379/1

# ============================================================================
# Notes
# ============================================================================
# 1. Never commit .env files to version control
# 2. Add .env to .gitignore
# 3. Use different .env files for different environments
# 4. Rotate API keys regularly
# 5. For air-gapped environments, use vLLM or llama.cpp with GLM-4.7 or MiniMax-M2.1
